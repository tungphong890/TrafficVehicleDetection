{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2655824,"sourceType":"datasetVersion","datasetId":1615118}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision albumentations opencv-python matplotlib tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T06:49:21.932021Z","iopub.execute_input":"2025-05-25T06:49:21.932372Z","iopub.status.idle":"2025-05-25T06:49:25.385660Z","shell.execute_reply.started":"2025-05-25T06:49:21.932346Z","shell.execute_reply":"2025-05-25T06:49:25.384705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.cuda.amp import GradScaler, autocast\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport torch.nn.functional as F\nfrom torchvision.ops import sigmoid_focal_loss, box_iou\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrafficDataset(Dataset):\n    def __init__(self, images_dir, labels_dir, transforms=None):\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.transforms = transforms\n        self.image_files = sorted([f for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.png'))])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.images_dir, self.image_files[idx])\n        img = np.array(Image.open(img_path).convert(\"RGB\"))\n        label_path = os.path.join(self.labels_dir, os.path.splitext(self.image_files[idx])[0] + '.txt')\n        boxes, labels = [], []\n        if os.path.exists(label_path):\n            with open(label_path) as f:\n                for line in f:\n                    cls, x_c, y_c, w, h = map(float, line.split())\n                    h_img, w_img = img.shape[:2]\n                    x_c, y_c = x_c * w_img, y_c * h_img\n                    w, h = w * w_img, h * h_img\n                    x0, y0 = x_c - w/2, y_c - h/2\n                    x1, y1 = x_c + w/2, y_c + h/2\n                    boxes.append([x0, y0, x1, y1])\n                    labels.append(int(cls) + 1)\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        if boxes.ndim == 1:\n            boxes = boxes.unsqueeze(0)\n        if boxes.numel() > 0:\n            valid = (boxes[:, 2] > boxes[:, 0]) & (boxes[:, 3] > boxes[:, 1])\n            boxes = boxes[valid]\n            labels = labels[valid]\n        if boxes.numel() == 0:\n            boxes = torch.zeros((0, 4), dtype=torch.float32)\n            labels = torch.zeros((0,), dtype=torch.int64)\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) if boxes.numel() else torch.zeros((0,), dtype=torch.float32)\n        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n        target = {\"boxes\": boxes, \"labels\": labels, \"image_id\": image_id, \"area\": area, \"iscrowd\": iscrowd}\n        if self.transforms:\n            labs = labels.tolist()\n            transformed = self.transforms(image=img, bboxes=target[\"boxes\"], labels=labs)\n            img = transformed[\"image\"]\n            target[\"boxes\"] = torch.as_tensor(transformed[\"bboxes\"], dtype=torch.float32)\n            target[\"labels\"] = torch.as_tensor(transformed[\"labels\"], dtype=torch.int64)\n        else:\n            img = ToTensorV2()(image=img)[\"image\"]\n        return img, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_transforms(train=True):\n    if train:\n        return A.Compose([\n            A.Resize(640, 640), A.HorizontalFlip(p=0.5), A.RandomBrightnessContrast(p=0.2),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n    else:\n        return A.Compose([\n            A.Resize(640, 640),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), ToTensorV2()\n        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    batch = [b for b in batch if b is not None]\n    return tuple(zip(*batch))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def det_precision_recall_f1(preds, targets, iou_thresh=0.5):\n    tp = fp = fn = 0\n    for pred, tgt in zip(preds, targets):\n        boxes_p, scores_p = pred['boxes'], pred['scores']\n        boxes_t = tgt['boxes']\n        keep = scores_p > 0.25\n        boxes_p = boxes_p[keep]\n        if boxes_p.numel() == 0 and boxes_t.numel() == 0:\n            continue\n        if boxes_p.numel() == 0:\n            fn += boxes_t.shape[0]\n            continue\n        if boxes_t.numel() == 0:\n            fp += boxes_p.shape[0]\n            continue\n        iou = box_iou(boxes_p, boxes_t)\n        matched_t = set()\n        for i in range(iou.shape[0]):\n            j = torch.argmax(iou[i])\n            if iou[i, j] >= iou_thresh and j.item() not in matched_t:\n                tp += 1\n                matched_t.add(j.item())\n            else:\n                fp += 1\n        fn += boxes_t.shape[0] - len(matched_t)\n    prec = tp / (tp + fp) if tp + fp > 0 else 0\n    rec = tp / (tp + fn) if tp + fn > 0 else 0\n    f1 = 2 * prec * rec / (prec + rec) if prec + rec > 0 else 0\n    return prec, rec, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main(args):\n    train_ds = TrafficDataset(args.train_images, args.train_labels, transforms=get_transforms(True))\n    val_ds   = TrafficDataset(args.val_images,   args.val_labels,   transforms=get_transforms(False))\n    counts = {i:0 for i in range(1,args.num_classes)}\n    img_w = []\n    for i in range(len(train_ds)):\n        _, tgt = train_ds[i]\n        labels = tgt['labels'].tolist()\n        for l in labels: counts[l]+=1\n        w = sum(1.0/counts[l] for l in set(labels)) if labels else 0\n        img_w.append(w)\n    sampler = WeightedRandomSampler(img_w, num_samples=len(img_w), replacement=True)\n    train_loader = DataLoader(train_ds, batch_size=args.batch_size, sampler=sampler,\n                              num_workers=args.num_workers, pin_memory=True, collate_fn=collate_fn)\n    val_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False,\n                              num_workers=args.num_workers, pin_memory=True, collate_fn=collate_fn)\n\n    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, args.num_classes)\n    def loss_with_focal(self, cls_logits, box_regression, labels, regression_targets):\n        cls_loss = sigmoid_focal_loss(\n            cls_logits.flatten(),\n            F.one_hot(labels, num_classes=self.cls_score.out_features).float().flatten(),\n            alpha=0.25, gamma=2.0, reduction='mean'\n        )\n        reg_loss = F.smooth_l1_loss(box_regression, regression_targets, reduction='sum') / cls_logits.shape[0]\n        return cls_loss + reg_loss\n    from types import MethodType\n    model.roi_heads.box_predictor.compute_loss = MethodType(loss_with_focal, model.roi_heads.box_predictor)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n\n    if args.scheduler == 'onecycle':\n        sched = torch.optim.lr_scheduler.OneCycleLR(\n            opt,\n            max_lr=args.lr,\n            steps_per_epoch=len(train_loader),\n            epochs=args.epochs\n        )\n    else:\n        sched = torch.optim.lr_scheduler.CosineAnnealingLR(\n            opt,\n            T_max=args.epochs\n        )\n\n    scaler = GradScaler()\n    map_metric = MeanAveragePrecision()\n    history = {'train_loss':[], 'val_loss':[], 'mAP50':[], 'mAP5095':[], 'precision':[], 'recall':[], 'f1':[], 'lr':[]}\n    best_loss = float('inf')\n    no_improve = {0:[], 'val_loss':[], 'mAP50':[], 'mAP5095':[], 'precision':[], 'recall':[], 'f1':[], 'lr':[]}\n    best_loss= float('inf'); no_improve=0\n\n    for epoch in range(1, args.epochs+1):\n        model.train(); tloss=0\n        loop = tqdm(train_loader, desc=f\"Train {epoch}/{args.epochs}\")\n        for step,(imgs,tgts) in enumerate(loop):\n            imgs = [i.to(device) for i in imgs]\n            tgts = [{k:v.to(device) for k,v in t.items()} for t in tgts]\n            with autocast(): loss = sum(model(imgs,tgts).values())\n            opt.zero_grad(); scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n            tloss+=loss.item(); loop.set_postfix(loss=tloss/(step+1))\n        sched.step()\n\n        vloss=0; model.train();\n        with torch.no_grad():\n            for imgs,tgts in val_loader:\n                imgs = [i.to(device) for i in imgs]\n                tgts = [{k:v.to(device) for k,v in t.items()} for t in tgts]\n                with autocast(): vloss+=sum(model(imgs,tgts).values()).item()\n        vloss/=len(val_loader)\n\n        model.eval(); all_preds,all_tgts=[],[]\n        with torch.no_grad():\n            for imgs,tgts in val_loader:\n                imgs = [i.to(device) for i in imgs]\n                tgts = [{k:v.to(device) for k,v in t.items()} for t in tgts]\n                out=model(imgs)\n                for o,t in zip(out,tgts): all_preds.append({'boxes':o['boxes'].cpu(),'scores':o['scores'].cpu(),'labels':o['labels'].cpu()}); all_tgts.append({'boxes':t['boxes'].cpu(),'labels':t['labels'].cpu()})\n                map_metric.update(all_preds, all_tgts)\n        mres=map_metric.compute(); prec,rec,f1 = det_precision_recall_f1(all_preds,all_tgts)\n\n        history['train_loss'].append(tloss/len(train_loader)); history['val_loss'].append(vloss)\n        history['mAP50'].append(mres['map_50'].item()); history['mAP5095'].append(mres['map'].item())\n        history['precision'].append(prec); history['recall'].append(rec); history['f1'].append(f1)\n        history['lr'].append(opt.param_groups[0]['lr'])\n\n        print(f\"Epoch {epoch}/{args.epochs} TLoss:{tloss/len(train_loader):.3f} VLoss:{vloss:.3f} mAP50:{mres['map_50']:.3f} mAP@[.50:.95]:{mres['map']:.3f} P/R/F1:{prec:.3f}/{rec:.3f}/{f1:.3f}\")\n        if tloss<best_loss: best_loss,no_improve=tloss,0; torch.save(model.state_dict(),os.path.join(args.output,'best.pth'))\n        else: no_improve+=1\n        if epoch%10==0: torch.save(model.state_dict(),os.path.join(args.output,f'epoch_{epoch}.pth'))\n        torch.save(model.state_dict(),os.path.join(args.output,'last.pth'))\n        if no_improve>=args.patience: print(f\"Early stopping at epoch {epoch}\"); break\n\n    return history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_history(history,output):\n    epochs=list(range(1,len(history['train_loss'])+1))\n    plt.figure(figsize=(8,5)); plt.plot(epochs, history['train_loss'], label='Train Loss'); plt.plot(epochs, history['val_loss'], label='Val Loss'); plt.title('Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.savefig(os.path.join(output,'loss.png')); plt.show()\n    plt.figure(figsize=(8,5)); plt.plot(epochs, history['mAP50'], label='mAP@50'); plt.plot(epochs, history['mAP5095'], label='mAP@[.50:.95]'); plt.title('mAP'); plt.xlabel('Epoch'); plt.ylabel('mAP'); plt.legend(); plt.savefig(os.path.join(output,'mAP.png')); plt.show()\n    plt.figure(figsize=(8,5)); plt.plot(epochs, history['precision'], label='Precision'); plt.plot(epochs, history['recall'], label='Recall'); plt.plot(epochs, history['f1'], label='F1'); plt.title('P/R/F1'); plt.xlabel('Epoch'); plt.ylabel('Value'); plt.legend(); plt.savefig(os.path.join(output,'prf1.png')); plt.show()\n    plt.figure(figsize=(8,5)); plt.plot(epochs, history['lr'], label='Learning Rate'); plt.title('LR'); plt.xlabel('Epoch'); plt.ylabel('LR'); plt.legend(); plt.savefig(os.path.join(output,'lr.png')); plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:16:30.251950Z","iopub.execute_input":"2025-05-25T07:16:30.252808Z","iopub.status.idle":"2025-05-25T07:18:22.369453Z","shell.execute_reply.started":"2025-05-25T07:16:30.252785Z","shell.execute_reply":"2025-05-25T07:18:22.368221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__=='__main__':\n    parser=argparse.ArgumentParser()\n    parser.add_argument('--train_images', default='/kaggle/input/traffic-vehicles-object-detection/Traffic Dataset/images/train')\n    parser.add_argument('--train_labels', default='/kaggle/input/traffic-vehicles-object-detection/Traffic Dataset/labels/train')\n    parser.add_argument('--val_images', default='/kaggle/input/traffic-vehicles-object-detection/Traffic Dataset/images/val')\n    parser.add_argument('--val_labels', default='/kaggle/input/traffic-vehicles-object-detection/Traffic Dataset/labels/val')\n    parser.add_argument('--output', default='checkpoints')\n    parser.add_argument('--epochs', type=int, default=100)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--lr', type=float, default=0.000909)\n    parser.add_argument('--weight_decay', type=float, default=0.0005)\n    parser.add_argument('--scheduler', choices=['onecycle','cosine'], default='onecycle')\n    parser.add_argument('--num_workers', type=int, default=0)\n    parser.add_argument('--patience', type=int, default=20)\n    parser.add_argument('--num_classes', type=int, default=8)\n    args,_=parser.parse_known_args()\n    os.makedirs(args.output, exist_ok=True)\n    history = main(args)\n    plot_history(history, args.output)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}